{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwM20wIatYjV",
        "outputId": "bb2c9844-260d-4a81-8a5a-9656214b320c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch numpy datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import logging\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Set environment variable for memory management\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(filename='gesal_training_logs.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logging.getLogger().addHandler(logging.StreamHandler())  # Log to console\n",
        "\n",
        "# Custom Linear Layer with SVF\n",
        "class SVFLinear(nn.Module):\n",
        "    def __init__(self, original_linear):\n",
        "        super().__init__()\n",
        "        self.original_linear = original_linear\n",
        "        with torch.no_grad():\n",
        "            U, Sigma, V = torch.svd(original_linear.weight.float())\n",
        "            self.U = nn.Parameter(U, requires_grad=False)\n",
        "            self.Sigma = nn.Parameter(Sigma, requires_grad=False)\n",
        "            self.V = nn.Parameter(V.t(), requires_grad=False)\n",
        "        self.z = nn.Parameter(torch.ones_like(self.Sigma), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Sigma_z = self.Sigma * self.z\n",
        "        Vx = torch.matmul(self.V, x.T if x.dim() == 2 else x.unsqueeze(-1))\n",
        "        Sigma_Vx = Sigma_z.unsqueeze(-1) * Vx\n",
        "        output = torch.matmul(self.U, Sigma_Vx)\n",
        "        if self.original_linear.bias is not None:\n",
        "            output = output + self.original_linear.bias.unsqueeze(-1)\n",
        "        return output.squeeze(-1) if x.dim() == 2 else output\n",
        "\n",
        "# Graph Node for Task-Specific Adaptations\n",
        "class Node:\n",
        "    def __init__(self, embedding, z_vectors, count=1):\n",
        "        self.embedding = embedding\n",
        "        self.z_vectors = z_vectors\n",
        "        self.count = count\n",
        "        self.past_responses = set()\n",
        "\n",
        "    def update_embedding(self, new_embedding):\n",
        "        self.embedding = (self.count * self.embedding + new_embedding) / (self.count + 1)\n",
        "        self.count += 1\n",
        "\n",
        "# Self-Adaptive LLM with Graph Structure\n",
        "class AdaptiveLLM:\n",
        "    def __init__(self, model_name=\"meta-llama/llama-3.2-1b-instruct\", distance_threshold=0.1, buffer_size=20, temperature=0.3, top_k=20, lr=0.005):\n",
        "        from huggingface_hub import login\n",
        "        login(token=\"XXXXXXXXXXXXXX\")  # Replace with your token\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.base_model.to(self.device)\n",
        "\n",
        "        self.svf_layers = []\n",
        "        for name, module in self.base_model.named_modules():\n",
        "            if isinstance(module, nn.Linear) and \"mlp\" in name:\n",
        "                svf_layer = SVFLinear(module)\n",
        "                self.svf_layers.append(svf_layer)\n",
        "                layer_idx = int(name.split(\".\")[2])\n",
        "                if \"c_fc\" in name:\n",
        "                    self.base_model.model.layers[layer_idx].mlp.c_fc = svf_layer\n",
        "                elif \"c_proj\" in name:\n",
        "                    self.base_model.model.layers[layer_idx].mlp.c_proj = svf_layer\n",
        "\n",
        "        initial_embedding = torch.zeros(2048).to(self.device)\n",
        "        initial_z_vectors = [layer.z.clone().detach() for layer in self.svf_layers]\n",
        "        self.nodes = [Node(initial_embedding, initial_z_vectors)]\n",
        "        self.buffer = []\n",
        "        self.distance_threshold = distance_threshold\n",
        "        self.buffer_size = buffer_size\n",
        "        self.temperature = temperature\n",
        "        self.top_k = top_k\n",
        "        self.lr = lr\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def embed_input(self, text, params=None):\n",
        "        if params:\n",
        "            params_array = np.array(list(params.values())).reshape(1, -1)\n",
        "            normalized_params = torch.tensor(self.scaler.fit_transform(params_array), device=self.device)\n",
        "            weights = {'wear': 0.5, 'exhaust_temp': 0.3, 'vibration': 0.2}\n",
        "            weighted_params = normalized_params * torch.tensor([weights.get(k, 0.0) for k in params.keys()], device=self.device)\n",
        "            text += f\" [PARAMS: {weighted_params}]\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.base_model(**inputs, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states[-1]\n",
        "            return torch.mean(hidden_states, dim=1).squeeze(0)\n",
        "\n",
        "    def compute_distance(self, emb1, emb2):\n",
        "        return 1 - F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
        "\n",
        "    def set_z_vectors(self, z_vectors):\n",
        "        for layer, z in zip(self.svf_layers, z_vectors):\n",
        "            layer.z.data = z.clone().to(self.device)\n",
        "\n",
        "    def process_input(self, text, params=None, feedback=None):\n",
        "        embedding = self.embed_input(text, params)\n",
        "        distances = [self.compute_distance(embedding, node.embedding) for node in self.nodes]\n",
        "        min_distance = min(distances)\n",
        "        closest_idx = np.argmin(distances)\n",
        "        closest_node = self.nodes[closest_idx]\n",
        "\n",
        "        if min_distance > self.distance_threshold:\n",
        "            new_z_vectors = [layer.z.clone().detach() for layer in self.svf_layers]\n",
        "            new_node = Node(embedding, new_z_vectors)\n",
        "            self.nodes.append(new_node)\n",
        "            logging.info(f\"New node created, total nodes: {len(self.nodes)}\")\n",
        "            closest_node = new_node\n",
        "        else:\n",
        "            closest_node.update_embedding(embedding)\n",
        "            logging.info(f\"Node {id(closest_node)} updated, embedding dist: {min_distance}\")\n",
        "\n",
        "        self.set_z_vectors(closest_node.z_vectors)\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "        outputs = self.base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            num_return_sequences=3,  # Reduced to 3 for memory efficiency\n",
        "            do_sample=True,\n",
        "            temperature=self.temperature,\n",
        "            top_k=self.top_k,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "        responses = [self.tokenizer.decode(out[inputs.input_ids.shape[1]:], skip_special_tokens=True).strip() for out in outputs]\n",
        "        response = max(set(responses), key=responses.count)  # Majority vote\n",
        "\n",
        "        while response in closest_node.past_responses and feedback == -2:\n",
        "            outputs = self.base_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                num_return_sequences=3,\n",
        "                do_sample=True,\n",
        "                temperature=self.temperature * 1.2,\n",
        "                top_k=self.top_k,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "            responses = [self.tokenizer.decode(out[inputs.input_ids.shape[1]:], skip_special_tokens=True).strip() for out in outputs]\n",
        "            response = max(set(responses), key=responses.count)\n",
        "\n",
        "        reward = 1  # Default positive\n",
        "        if params and 'wear' in params:\n",
        "            wear = float(params['wear'])\n",
        "            if wear > 3.0 and 'replace' not in response.lower():\n",
        "                reward = -2\n",
        "            elif 1.5 <= wear <= 3.0 and 'maintenance' not in response.lower():\n",
        "                reward = -2\n",
        "            elif wear < 1.5 and 'check' not in response.lower():\n",
        "                reward = -2\n",
        "            elif feedback is not None:\n",
        "                reward = feedback\n",
        "\n",
        "        closest_node.past_responses.add(response)\n",
        "        self.buffer.append((text, response, reward, closest_node))\n",
        "        logging.info(f\"Generated response: {response}, Reward: {reward}, Nodes: {len(self.nodes)}\")\n",
        "\n",
        "        if len(self.buffer) >= self.buffer_size:\n",
        "            self.update_nodes()\n",
        "\n",
        "        return response\n",
        "\n",
        "    def update_nodes(self):\n",
        "        node_data = defaultdict(list)\n",
        "        for text, response, reward, node in self.buffer:\n",
        "            node_data[id(node)].append((text, response, reward))\n",
        "\n",
        "        for node in self.nodes:\n",
        "            data = node_data.get(id(node), [])\n",
        "            if data:\n",
        "                optimizers = [torch.optim.Adam([z], lr=self.lr) for z in node.z_vectors]\n",
        "                for text, response, reward in data:\n",
        "                    full_text = text + \" \" + response\n",
        "                    inputs = self.tokenizer(full_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "                    input_len = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.shape[1]\n",
        "                    targets = self.tokenizer(response, return_tensors=\"pt\", padding=True, truncation=True, max_length=50).input_ids.to(self.device)\n",
        "\n",
        "                    self.set_z_vectors(node.z_vectors)\n",
        "                    outputs = self.base_model(**inputs)\n",
        "                    logits = outputs.logits[:, input_len-1:-1, :]\n",
        "                    if logits.shape[1] < targets.shape[1]:\n",
        "                        padding = torch.zeros((1, targets.shape[1] - logits.shape[1], logits.shape[2]), device=self.device)\n",
        "                        logits = torch.cat([logits, padding], dim=1)\n",
        "                    elif logits.shape[1] > targets.shape[1]:\n",
        "                        logits = logits[:, :targets.shape[1], :]\n",
        "\n",
        "                    log_probs = F.log_softmax(logits, dim=-1)\n",
        "                    target_log_probs = log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)\n",
        "                    loss = -reward * target_log_probs.mean()\n",
        "                    for opt in optimizers:\n",
        "                        opt.zero_grad()\n",
        "                    loss.backward()\n",
        "                    for opt in optimizers:\n",
        "                        opt.step()\n",
        "                logging.info(f\"Node {id(node)} updated with loss: {loss.item()}\")\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory after each node update\n",
        "        self.buffer = []\n",
        "        torch.cuda.empty_cache()  # Clear GPU memory after buffer processing\n",
        "\n",
        "# Mock InfluxDB data (replace with actual integration)\n",
        "def get_engine_data(engine_id):\n",
        "    data = {\n",
        "        \"flight_hours\": random.uniform(5000, 8000),\n",
        "        \"exhaust_temp\": random.uniform(600, 750),\n",
        "        \"vibration\": random.uniform(0.5, 12.5),\n",
        "        \"thrust\": random.uniform(50, 75),\n",
        "        \"pressure\": random.uniform(1.0, 13561),\n",
        "        \"wear\": random.uniform(0.5, 4.0)\n",
        "    }\n",
        "    return data\n",
        "\n",
        "# Function to evaluate on engine dataset\n",
        "def evaluate_engine(model, num_engines=1326, batch_size=100):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_start in range(0, num_engines, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, num_engines)\n",
        "        for i in range(batch_start, batch_end):\n",
        "            params = get_engine_data(i)\n",
        "            wear = params['wear']\n",
        "            prompt = (f\"Given engine with flight_hours={params['flight_hours']:.2f}h, \"\n",
        "                      f\"exhaust_temp={params['exhaust_temp']:.2f}°C, vibration={params['vibration']:.2f}g, \"\n",
        "                      f\"thrust={params['thrust']:.2f}kN, pressure={params['pressure']:.2f}bar, \"\n",
        "                      f\"wear={wear:.2f}, predict maintenance action: ‘replace’, ‘maintenance’, or ‘check’ \"\n",
        "                      f\"based on wear thresholds: wear > 3.0 = replace, 1.5–3.0 = maintenance, <1.5 = check.\")\n",
        "            response = model.process_input(prompt, params)\n",
        "            try:\n",
        "                if wear > 3.0 and 'replace' in response.lower():\n",
        "                    correct += 1\n",
        "                elif 1.5 <= wear <= 3.0 and 'maintenance' in response.lower():\n",
        "                    correct += 1\n",
        "                elif wear < 1.5 and 'check' in response.lower():\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "                feedback = 1 if (wear > 3.0 and 'replace' in response.lower()) or \\\n",
        "                               (1.5 <= wear <= 3.0 and 'maintenance' in response.lower()) or \\\n",
        "                               (wear < 1.5 and 'check' in response.lower()) else -2\n",
        "                model.process_input(prompt, params, feedback)\n",
        "                logging.info(f\"Engine {i}: Predicted {response}, True {get_true_action(wear)}, Correct: {feedback == 1}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing engine {i}: {e}\")\n",
        "                total += 1\n",
        "            torch.cuda.empty_cache()  # Clear GPU memory after each engine\n",
        "        logging.info(f\"Completed batch {batch_start}-{batch_end-1}\")\n",
        "    accuracy = correct / total * 100\n",
        "    logging.info(f\"Evaluation complete. Accuracy: {accuracy}% on {total} engines\")\n",
        "    with open('gesal_evaluation_results.txt', 'w') as f:\n",
        "        f.write(f\"Accuracy: {accuracy}%\\nNodes: {len(model.nodes)}\\n\")\n",
        "    logging.getLogger().handlers[0].flush()\n",
        "    return accuracy\n",
        "\n",
        "def get_true_action(wear):\n",
        "    if wear > 3.0:\n",
        "        return \"replace\"\n",
        "    elif 1.5 <= wear <= 3.0:\n",
        "        return \"maintenance\"\n",
        "    else:\n",
        "        return \"check\"\n",
        "\n",
        "# Warm-up phase\n",
        "def warm_up(model, num_warmup=5):\n",
        "    print(\"Starting warm-up phase...\")\n",
        "    for i in range(num_warmup):\n",
        "        params = get_engine_data(i)\n",
        "        prompt = (f\"Given engine with flight_hours={params['flight_hours']:.2f}h, \"\n",
        "                  f\"exhaust_temp={params['exhaust_temp']:.2f}°C, vibration={params['vibration']:.2f}g, \"\n",
        "                  f\"thrust={params['thrust']:.2f}kN, pressure={params['pressure']:.2f}bar, \"\n",
        "                  f\"wear={params['wear']:.2f}, predict maintenance action: ‘replace’, ‘maintenance’, or ‘check’ \"\n",
        "                  f\"based on wear thresholds: wear > 3.0 = replace, 1.5–3.0 = maintenance, <1.5 = check.\")\n",
        "        response = model.process_input(prompt, params)\n",
        "        true_action = get_true_action(params['wear'])\n",
        "        feedback = 1 if true_action in response.lower() else -2\n",
        "        model.process_input(prompt, params, feedback)\n",
        "        logging.info(f\"Warm-up {i}: Predicted {response}, True {true_action}, Feedback: {feedback}\")\n",
        "    print(\"Warm-up complete. Beginning full evaluation...\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize model\n",
        "    model = AdaptiveLLM(\n",
        "        distance_threshold=0.1,\n",
        "        buffer_size=20,\n",
        "        temperature=0.3,\n",
        "        top_k=20,\n",
        "        lr=0.005\n",
        "    )\n",
        "\n",
        "    # Warm-up phase\n",
        "    warm_up(model)\n",
        "\n",
        "    # Run full evaluation\n",
        "    print(\"Starting GESAL evaluation on engine dataset...\")\n",
        "    accuracy = evaluate_engine(model, batch_size=100)\n",
        "    print(f\"Final Accuracy on Engine Dataset: {accuracy}%\")\n",
        "    print(f\"Total Nodes Created: {len(model.nodes)}\")\n",
        "\n",
        "    # Display logs and results\n",
        "    try:\n",
        "        with open('gesal_training_logs.txt', 'r') as f:\n",
        "            print(\"Logs:\")\n",
        "            print(f.read())\n",
        "        with open('gesal_evaluation_results.txt', 'r') as f:\n",
        "            print(\"Results:\")\n",
        "            print(f.read())\n",
        "    except FileNotFoundError:\n",
        "        print(\"Warning: Log or results file not found. Check for errors above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyjsa-YYttaJ",
        "outputId": "ac2febc2-57ca-44d2-c77b-446913e0ef92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting warm-up phase...\n",
            "Warm-up complete. Beginning full evaluation...\n",
            "Starting GESAL evaluation on engine dataset...\n",
            "Final Accuracy on Engine Dataset: 40.79939668174962%\n",
            "Total Nodes Created: 2\n",
            "Warning: Log or results file not found. Check for errors above.\n"
          ]
        }
      ]
    }
  ]
}
